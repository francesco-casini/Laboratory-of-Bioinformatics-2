{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1hXdPSJ9Of6AyV1h_lBs5dR5iEs8TO4QZ","timestamp":1730321532564},{"file_id":"1-ysY0gK-nXzmIDLr0T2hgklxAHnOr7FA","timestamp":1730308132421},{"file_id":"1XogPT2wEZcOGKYfNX32IgZnE6prYB5At","timestamp":1729781083062},{"file_id":"1UaWuQjeiI8Jf2t_1XRS6-Z3t5nx4SP7Z","timestamp":1729695734938},{"file_id":"1gsI8rltMRsJOfZR2ilaKIyOL13FZQmMG","timestamp":1729685138572}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"source":["#@title Install necessary libraries\n","!pip install transformers[torch]  # Install transformers with PyTorch support\n","!pip install biopython  # Install Biopython for sequence handling"],"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2eEYWwsrrw1b","executionInfo":{"status":"ok","timestamp":1730366352519,"user_tz":-60,"elapsed":8270,"user":{"displayName":"Francesco Casini","userId":"11464238357026466333"}},"outputId":"61e59980-8546-412a-825f-917083e78246"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.44.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.16.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.24.7)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.5)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.5)\n","Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.34.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.5.0+cu121)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (2024.6.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.4)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->transformers[torch]) (1.3.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.8.30)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (3.0.2)\n","Requirement already satisfied: biopython in /usr/local/lib/python3.10/dist-packages (1.84)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython) (1.26.4)\n"]}]},{"cell_type":"markdown","source":["# -- **giusto**"],"metadata":{"id":"q0noImZ1NXkx"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from sklearn.metrics import matthews_corrcoef\n","from torch.utils.data import DataLoader, TensorDataset\n","from Bio import SeqIO\n","from tqdm import tqdm\n","from transformers import EsmTokenizer, EsmForSequenceClassification\n","import random\n","import logging\n","\n"],"metadata":{"id":"eBt0dm6rOEpD","executionInfo":{"status":"ok","timestamp":1730366367265,"user_tz":-60,"elapsed":14748,"user":{"displayName":"Francesco Casini","userId":"11464238357026466333"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Load the tokenizer and model\n","model_name = \"facebook/esm2_t33_650M_UR50D\"\n","tokenizer = EsmTokenizer.from_pretrained(model_name)\n","model = EsmForSequenceClassification.from_pretrained(model_name)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)"],"metadata":{"id":"55nXB6dVOEvB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730366371062,"user_tz":-60,"elapsed":3801,"user":{"displayName":"Francesco Casini","userId":"11464238357026466333"}},"outputId":"3d1c17d4-33a8-487e-83a5-340fd506e268"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n","Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["EsmForSequenceClassification(\n","  (esm): EsmModel(\n","    (embeddings): EsmEmbeddings(\n","      (word_embeddings): Embedding(33, 1280, padding_idx=1)\n","      (dropout): Dropout(p=0.0, inplace=False)\n","      (position_embeddings): Embedding(1026, 1280, padding_idx=1)\n","    )\n","    (encoder): EsmEncoder(\n","      (layer): ModuleList(\n","        (0-32): 33 x EsmLayer(\n","          (attention): EsmAttention(\n","            (self): EsmSelfAttention(\n","              (query): Linear(in_features=1280, out_features=1280, bias=True)\n","              (key): Linear(in_features=1280, out_features=1280, bias=True)\n","              (value): Linear(in_features=1280, out_features=1280, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","              (rotary_embeddings): RotaryEmbedding()\n","            )\n","            (output): EsmSelfOutput(\n","              (dense): Linear(in_features=1280, out_features=1280, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","            (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","          )\n","          (intermediate): EsmIntermediate(\n","            (dense): Linear(in_features=1280, out_features=5120, bias=True)\n","          )\n","          (output): EsmOutput(\n","            (dense): Linear(in_features=5120, out_features=1280, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","          (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","      (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","    )\n","    (contact_head): EsmContactPredictionHead(\n","      (regression): Linear(in_features=660, out_features=1, bias=True)\n","      (activation): Sigmoid()\n","    )\n","  )\n","  (classifier): EsmClassificationHead(\n","    (dense): Linear(in_features=1280, out_features=1280, bias=True)\n","    (dropout): Dropout(p=0.0, inplace=False)\n","    (out_proj): Linear(in_features=1280, out_features=2, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["# Load training and benchmark data\n","pos_train = pd.read_csv('pos_training.tsv', sep='\\t', header=None, names=[\"Accession\", \"Organism\", \"Kingdom\", \"Length\", \"Cleavage\", \"Type\", \"Fold\"])\n","neg_train = pd.read_csv('neg_training.tsv', sep='\\t', header=None, names=[\"Accession\", \"Organism\", \"Kingdom\", \"Length\", \"Transmembrane\", \"Type\", \"Fold\"])\n","\n","pos_bench = pd.read_csv('pos_benchmark.tsv', sep='\\t', header=None, names=['Accession', 'Organism', 'Kingdom', 'Length', 'Cleavage', 'Type'])\n","neg_bench = pd.read_csv('neg_benchmark.tsv', sep='\\t', header=None, names=['Accession', 'Organism', 'Kingdom', 'Length', 'Transmembrane', 'Type'])\n","\n","# Combine training and benchmark data\n","full_training_df = pd.concat([pos_train, neg_train], ignore_index=True)\n","full_benchmark_df = pd.concat([pos_bench, neg_bench], ignore_index=True)\n","\n","# Create label column based on 'Type'\n","full_training_df['Label'] = full_training_df['Type'].apply(lambda x: 1 if x == '+' else 0)\n","full_benchmark_df['Label'] = full_benchmark_df['Type'].apply(lambda x: 1 if x == '+' else 0)\n","\n"],"metadata":{"id":"3DZc7BCWNbdp","executionInfo":{"status":"ok","timestamp":1730366371063,"user_tz":-60,"elapsed":7,"user":{"displayName":"Francesco Casini","userId":"11464238357026466333"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def parse_fasta(fasta_file):\n","    fasta_dict = {}\n","    with open(fasta_file, \"r\") as fasta:\n","        current_id, current_seq = \"\", \"\"\n","\n","        for line in fasta:\n","            line = line.strip()\n","            if line.startswith(\">\"):\n","                if current_id:\n","                    fasta_dict[current_id] = current_seq\n","                current_id = line[1:]  # Remove the '>' character\n","                current_seq = \"\"\n","            else:\n","                current_seq += line\n","\n","        if current_id:\n","            fasta_dict[current_id] = current_seq  # Add the last record\n","\n","    return fasta_dict"],"metadata":{"id":"SadxYx14O1XE","executionInfo":{"status":"ok","timestamp":1730366371063,"user_tz":-60,"elapsed":6,"user":{"displayName":"Francesco Casini","userId":"11464238357026466333"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Parse the FASTA files for positive and negative sequences\n","total_pos_fasta = parse_fasta(\"pos_cluster-results_rep_seq.fasta\")\n","total_neg_fasta = parse_fasta(\"neg_cluster-results_rep_seq.fasta\")\n","\n","# Map sequences from FASTA files to the DataFrames\n","full_training_df['Sequence'] = full_training_df['Accession'].map(total_pos_fasta).combine_first(\n","    full_training_df['Accession'].map(total_neg_fasta))\n","\n","full_benchmark_df['Sequence'] = full_benchmark_df['Accession'].map(total_pos_fasta).combine_first(\n","    full_benchmark_df['Accession'].map(total_neg_fasta))\n","\n","# Save updated DataFrames to new files\n","full_training_df.to_csv('full_training_with_sequences.tsv', sep='\\t', index=False)\n","full_benchmark_df.to_csv('full_benchmark_with_sequences.tsv', sep='\\t', index=False)"],"metadata":{"id":"XXvU7yqmhL1t","executionInfo":{"status":"ok","timestamp":1730366371428,"user_tz":-60,"elapsed":371,"user":{"displayName":"Francesco Casini","userId":"11464238357026466333"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Function to compute embeddings\n","def compute_embeddings(sequence):\n","  inputs = tokenizer(sequence, return_tensors=\"pt\").to(device)\n","  outputs = model(**inputs, output_hidden_states=True)\n","  last_hidden_state = outputs.hidden_states[-1]  # Last layer\n","  embeddings = last_hidden_state[0, 1:91, :] # Extract embeddings from the last layer\n","  return embeddings\n","\n"],"metadata":{"id":"-xya88CUgSxr","executionInfo":{"status":"ok","timestamp":1730366371428,"user_tz":-60,"elapsed":3,"user":{"displayName":"Francesco Casini","userId":"11464238357026466333"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger(__name__)\n","\n","# Function to extract embeddings and labels\n","def extract_XY(df):\n","    M, Y = [], []\n","    folds = df.get('Fold', None).values if 'Fold' in df.columns else None\n","\n","    total_rows = df.shape[0]\n","    logger.info(f\"Starting to process {total_rows} records.\")\n","\n","    # Process sequences from the DataFrame\n","    for index, row in tqdm(df.iterrows(), total=total_rows, desc=\"Processing records\"):\n","        sequence = row['Sequence']\n","\n","        # Check if sequence is a valid string\n","        if isinstance(sequence, str):\n","            sequence = sequence[:1022]  # Ensure slicing only for string types\n","        else:\n","            logger.warning(f\"Skipping row {index} due to invalid sequence type: {type(sequence)}\")\n","            continue  # Skip this iteration if the sequence is not valid\n","\n","        label = row['Label']\n","\n","        # Compute embeddings\n","        embedding = compute_embeddings(sequence)\n","        mean_embedding = np.mean(embedding.detach().cpu().numpy(), axis=0)  # Average embedding\n","\n","        # Append results\n","        M.append(mean_embedding)\n","        Y.append(label)\n","\n","    logger.info(\"Processing completed successfully.\")\n","\n","    # Return folds only if they were present in the DataFrame\n","    if folds:\n","        return np.array(M), np.array(Y), folds\n","    else:\n","        return np.array(M), np.array(Y)"],"metadata":{"id":"RwAOa_LzNSS5","executionInfo":{"status":"ok","timestamp":1730366371428,"user_tz":-60,"elapsed":2,"user":{"displayName":"Francesco Casini","userId":"11464238357026466333"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["train_M, train_Y, train_folds = extract_XY(full_training_df)\n"],"metadata":{"id":"bNtWqCTBN7Q0","colab":{"base_uri":"https://localhost:8080/"},"outputId":"056ffd68-2571-45a0-b48c-fe6a6c44d630"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Processing records:   0%|          | 0/7901 [00:00<?, ?it/s]WARNING:__main__:Skipping row 0 due to invalid sequence type: <class 'float'>\n","Processing records:   0%|          | 3/7901 [01:03<51:48:26, 23.61s/it]"]}]},{"cell_type":"code","source":["benchmark_M, benchmark_Y = extract_XY(full_benchmark_df)\n"],"metadata":{"id":"eR1YERi8qLwY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#  Combine training embeddings into a DataFrame\n","training_data = pd.DataFrame(train_M)\n","training_data['Label'] = train_Y\n","training_data['Fold'] = train_folds  # Only add fold data for the training set\n","\n","# Combine benchmark embeddings into a separate DataFrame\n","benchmark_data = pd.DataFrame(benchmark_M)\n","benchmark_data['Label'] = benchmark_Y  # Labels for benchmark data\n","\n","# Save DataFrames if needed\n","training_data.to_csv('full_training_with_embeddings.csv', index=False)\n","benchmark_data.to_csv('full_benchmark_with_embeddings.csv', index=False)\n"],"metadata":{"id":"VZ_00uKaN8kj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Layer Construction:\n","\n","Layers list: layers = [] creates an empty list to store each layer.\n","Loop through hidden_layer_sizes: For each size in hidden_layer_sizes, the following layers are added in sequence:\n","*   nn.Linear(in_features, size): A fully connected layer where the input has in_features neurons, and the output has size neurons.\n","*   nn.ReLU(): A ReLU activation function to introduce non-linearity.\n","*   nn.Dropout(dropout_p): A dropout layer to randomly set some neurons' output to zero, helping prevent overfitting.\n","\n","\n","After each loop iteration, in_features is updated to size, preparing it for the next hidden layer.\n","\n","logits = self.model(x) passes the input x through all layers in self.model\n","\n","Apply Sigmoid: torch.sigmoid(logits) applies the Sigmoid, which maps the output to values between 0 and 1 (binary class)"],"metadata":{"id":"bkW5q-uKyDWD"}},{"cell_type":"code","source":["class SPMLP(nn.Module):\n","    def __init__(self, input_size, hidden_layer_sizes, output_size, dropout_p=0.5):\n","        super(SPMLP, self).__init__()\n","        layers = []\n","        in_features = input_size\n","\n","        for size in hidden_layer_sizes:\n","            layers.append(nn.Linear(in_features, size))\n","            layers.append(nn.ReLU())\n","            layers.append(nn.Dropout(dropout_p))\n","            in_features = size\n","\n","        layers.append(nn.Linear(in_features, output_size))  # Output layer\n","        self.model = nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        logits = self.model(x)\n","        return torch.sigmoid(logits)  # Use sigmoid for output\n","\n","class SPDataset(Dataset):\n","    def __init__(self, X, y):\n","        self.X = torch.tensor(X, dtype=torch.float32)\n","        # Convert string labels to numerical labels for categorical classification\n","        unique_labels = np.unique(y)\n","        label_mapping = {label: i for i, label in enumerate(unique_labels)}\n","        labels = np.array([label_mapping[label] for label in y])\n","        self.y = torch.tensor(labels, dtype=torch.long)\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, idx):\n","        return self.X[idx], self.y[idx]"],"metadata":{"id":"hdZGRr9cOLUP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ModelArchitectureManager:\n","    def __init__(self, device='cpu'):\n","        self.best_model_state_dict = None\n","        self.best_score = -1\n","        self.best_hidden_layer_sizes = None\n","        self.device = device\n","        logging.basicConfig(level=logging.INFO)\n","\n","    def _generate_random_hyperparameters(self):\n","        num_layers = random.randint(1, 5)\n","        hidden_layer_sizes = [random.randint(20, 100) for _ in range(num_layers)]\n","        dropout_p = random.uniform(0.1, 0.5)\n","        learning_rate = random.uniform(1e-5, 1e-2)\n","        return num_layers, hidden_layer_sizes, dropout_p, learning_rate\n","\n","    def train_model(self, model, train_loader, criterion, optimizer, epochs):\n","        model.train()\n","        for epoch in range(epochs):\n","            for batch_X, batch_y in train_loader:\n","                batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)\n","                optimizer.zero_grad()\n","                outputs = model(batch_X)\n","                loss = criterion(outputs, batch_y.float())\n","                loss.backward()\n","                optimizer.step()\n","\n","    def evaluate_model(self, model, val_loader):\n","        model.eval()\n","        all_preds = []\n","        all_labels = []\n","        with torch.no_grad():\n","            for batch_X, batch_y in val_loader:\n","                batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)\n","                outputs = model(batch_X)\n","                preds = torch.round(outputs).cpu().numpy()\n","                all_preds.extend(preds)\n","                all_labels.extend(batch_y.cpu().numpy())\n","        return matthews_corrcoef(all_labels, all_preds)\n","\n","    def random_search(self, num_trials, input_size, output_size, train_loader, val_loader, epochs=100):\n","        best_mcc = -1\n","        best_model_state = None\n","\n","        for trial in range(num_trials):\n","            logging.info(f\"Random Search Trial {trial + 1}/{num_trials}\")\n","\n","            num_layers, hidden_layer_sizes, dropout_p, learning_rate = self._generate_random_hyperparameters()\n","\n","            model = SPMLP(input_size, hidden_layer_sizes, output_size, dropout_p=dropout_p).to(self.device)\n","            criterion = nn.BCEWithLogitsLoss()\n","            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","            self.train_model(model, train_loader, criterion, optimizer, epochs=epochs)\n","\n","            mcc = self.evaluate_model(model, val_loader)\n","            logging.info(f\"Trial {trial + 1}: Validation MCC: {mcc:.4f}\")\n","\n","            if mcc > best_mcc:\n","                best_mcc = mcc\n","                best_model_state = model.state_dict()\n","                self.best_hidden_layer_sizes = hidden_layer_sizes  # Store best architecture sizes\n","\n","        self.best_model_state_dict = best_model_state\n","        self.best_score = best_mcc\n","        logging.info(f\"Best MCC: {best_mcc:.4f} with hidden layers: {self.best_hidden_layer_sizes}\")\n","        return self.best_model_state_dict, self.best_score\n","\n","    def test_model(self, model, test_loader):\n","        model.eval()\n","        all_preds = []\n","        all_labels = []\n","        with torch.no_grad():\n","            for batch_X, batch_y in test_loader:\n","                batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)\n","                outputs = model(batch_X)\n","                preds = torch.round(outputs).cpu().numpy()\n","                all_preds.extend(preds)\n","                all_labels.extend(batch_y.cpu().numpy())\n","        return matthews_corrcoef(all_labels, all_preds)\n"],"metadata":{"id":"Dyyk3lZ1OIhu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def find_best_model(combined_data):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    manager = ModelArchitectureManager(device=device)\n","\n","    input_size = combined_data.shape[1] - 2  # Adjusting for label and Fold columns\n","    output_size = 1  # Output size should be 1 for binary classification\n","    all_mcc_scores = []\n","\n","    for fold in range(combined_data['Fold'].nunique()):\n","        logging.info(f\"\\nFold {fold + 1}\")\n","\n","        # Define test, validation, and training data based on the Fold column\n","        test_data = combined_data[combined_data['Fold'] == fold]\n","        val_data = combined_data[combined_data['Fold'] == (fold + 1) % combined_data['Fold'].nunique()]\n","        train_data = combined_data[combined_data['Fold'] != fold]\n","        train_data = train_data[train_data['Fold'] != (fold + 1) % combined_data['Fold'].nunique()]\n","\n","        # Create DataLoaders using SPDataset\n","        train_dataset = SPDataset(train_data.iloc[:, :-2].values, train_data.iloc[:, -1].values)\n","        val_dataset = SPDataset(val_data.iloc[:, :-2].values, val_data.iloc[:, -1].values)\n","        test_dataset = SPDataset(test_data.iloc[:, :-2].values, test_data.iloc[:, -1].values)\n","\n","        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n","        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","\n","        # Optimize hyperparameters using the validation fold\n","        manager.random_search(\n","            num_trials=10,\n","            input_size=input_size,\n","            output_size=output_size,\n","            train_loader=train_loader,\n","            val_loader=val_loader\n","        )\n","\n","        # Create the best model with the best architecture\n","        best_model = SPMLP(input_size, manager.best_hidden_layer_sizes, output_size).to(device)\n","        best_model.load_state_dict(manager.best_model_state_dict)\n","\n","        # Test the best model on the test fold\n","        test_score = manager.evaluate_model(best_model, test_loader)\n","        logging.info(f\"Best architecture for Fold {fold + 1}: {manager.best_hidden_layer_sizes} with Test MCC: {test_score:.4f}\")\n","\n","        all_mcc_scores.append(test_score)\n","\n","    # Print results summary\n","    logging.info(f\"\\nAll MCC scores: {all_mcc_scores}\")\n","    logging.info(f\"Average MCC: {np.mean(all_mcc_scores):.4f}\")\n","\n","\n"],"metadata":{"id":"h0Dc5EUAnq7l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Find the best model\n","find_best_model(training_data)\n","\n","# After finding the best model, test it on the benchmark data\n","final_model = SPMLP(input_size, manager.best_hidden_layer_sizes, output_size).to(device)\n","final_model.load_state_dict(manager.best_model_state_dict)\n","\n","# Create DataLoader for benchmark dataset\n","benchmark_dataset = SPDataset(benchmark_data.iloc[:, :-2].values, benchmark_data.iloc[:, -1].values)\n","benchmark_loader = DataLoader(benchmark_dataset, batch_size=32, shuffle=False)\n","\n","# Evaluate the best model on the benchmark data using the new test method\n","benchmark_score = manager.test_model(final_model, benchmark_loader)\n","print(f\"Benchmark Model MCC: {benchmark_score:.4f}\")"],"metadata":{"id":"rIpi5mezn8NR"},"execution_count":null,"outputs":[]}]}